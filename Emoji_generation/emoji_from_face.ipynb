{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mediapipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_facial_expression(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image\n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        return \"neutral\"  # Default if no face detected\n",
    "\n",
    "    # Extract key points (eyes, eyebrows, mouth)\n",
    "    face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "    # Define key landmark indices for facial expressions\n",
    "    left_eye_top = face_landmarks.landmark[159]  # Upper eyelid\n",
    "    left_eye_bottom = face_landmarks.landmark[145]  # Lower eyelid\n",
    "    mouth_top = face_landmarks.landmark[13]\n",
    "    mouth_bottom = face_landmarks.landmark[14]\n",
    "\n",
    "    # Calculate distances for eye openness and mouth openness\n",
    "    eye_openness = abs(left_eye_top.y - left_eye_bottom.y)\n",
    "    mouth_openness = abs(mouth_top.y - mouth_bottom.y)\n",
    "\n",
    "    # Classify emotion based on distances\n",
    "    if mouth_openness > 0.04:\n",
    "        return \"surprise\"\n",
    "    elif eye_openness < 0.02:\n",
    "        return \"angry\"\n",
    "    elif mouth_openness < 0.02:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotion: neutral\n"
     ]
    }
   ],
   "source": [
    "image_path = \"face.jpg\"\n",
    "emotion = detect_facial_expression(image_path)\n",
    "print(\"Detected Emotion:\", emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_emoji = {\n",
    "    \"happy\": \"ğŸ˜Š\",\n",
    "    \"neutral\": \"ğŸ˜\",\n",
    "    \"angry\": \"ğŸ˜¡\",\n",
    "    \"surprise\": \"ğŸ˜®\"\n",
    "}\n",
    "\n",
    "def get_emoji_from_emotion(emotion):\n",
    "    return emotion_to_emoji.get(emotion, \"ğŸ˜\")  # Default to neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Emoji: ğŸ˜\n"
     ]
    }
   ],
   "source": [
    "emoji = get_emoji_from_emotion(emotion)\n",
    "print(\"Generated Emoji:\", emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully captured emotion from images and created emoji for that emotion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
